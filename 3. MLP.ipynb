{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260c6f2d",
   "metadata": {},
   "source": [
    "# Classification using MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d88bb",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will be using Covertype dataset available in dataset folder to test MLP.\n",
    "<br>We are supposed to predict `Forest Cover type` using following features\n",
    "\n",
    "<div>\n",
    "<img src=\"img/Dataset.png\" width=\"800\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73ed5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make + as string concat operator as well.. Dont worry about this cell...\n",
    "\n",
    "\"+\" = function(x,y) {\n",
    "    if(is.character(x) || is.character(y)) return(paste(x , y, sep=\"\"))\n",
    "    else .Primitive(\"+\")(x,y)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eed64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a1951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = read.csv(\"dataset/covtype.data\", header=FALSE, stringsAsFactors=FALSE)\n",
    "colnames(d1) = c('Elevation', 'Aspect', 'Slope', 'HorHydro', 'VertHydro', 'HorRoad', 'Shade9', 'Shade12', 'Shade15', 'HorFire', 'WildernessArea'+(1:4), 'SoilType'+(1:40), 'CoverType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  =  d1[  , -ncol(d1) ]\n",
    "train_labels = d1[  ,  ncol(d1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be972a0d",
   "metadata": {},
   "source": [
    "#### Input Data Format\n",
    "Keras requires the input data to be matrix with features as columns and instance as rows.\n",
    "<br>The response variable is required to be a binary matrix with each class in a seraparate column, which is achieved using `to_categorical` function of keras. `to_categorical` requires the labels to start from `0`. But since in our dataset it start with `1` we, subtract its value by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = as.matrix(train_data)\n",
    "train_labels = to_categorical(train_labels-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac313a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/Network.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89b7a9",
   "metadata": {},
   "source": [
    "#### Linear Stack of Layers\n",
    "Keras Model composed of Layers of compute units, most common of which is a Linear Stack of Layer.\n",
    "<br> We define a Linear stack using \"keras_model_sequential\" functions.\n",
    "<br> We then add layer by layer to the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_model_sequential() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3b724",
   "metadata": {},
   "source": [
    "#### Input Layer\n",
    "The first layer of MLP is the input layer which receives the input data.\n",
    "<br> The total number of nodes in this layer is equal to the total number of input features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e0782",
   "metadata": {},
   "source": [
    "#### Hidden Layer\n",
    "Hidden layer is where the computation takes places. There are many parameters in hidden layer which changes the performance of the model, few of which are `activation function`, `number of hidden layers in a nerwork`, `number of hidden units in a layer`. The below shows an example with 2 hidden layers and 200 units in each hidden layer, with `relu` activation function for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model  %>% layer_dense(\n",
    "    input_shape = ncol(train_data), \n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model  %>% layer_dense(\n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a21192",
   "metadata": {},
   "source": [
    "#### Output Layer\n",
    "Output layer is the last layer of the network, which gives the prediction.\n",
    "<br> The nature of the `activation function` tells if the prediction is classification or regression. For classification there are couple of options, with `softmax` being one of them for multi-class classification. \n",
    "<br> Total number of units in output class should be equal to the total number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c62fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model  %>% layer_dense(units = ncol(train_labels), activation = 'softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a849e",
   "metadata": {},
   "source": [
    "#### Configure Loss Function, Optimizer Algorithm, Error Metric using `compile` function\n",
    "\n",
    "- Loss Functions - https://keras.io/api/losses/\n",
    "- Optimizer Algorithms - https://keras.io/api/optimizers/\n",
    "- Error Metrics - https://keras.io/api/metrics/ \n",
    "\n",
    "compile R function - https://keras.rstudio.com/reference/compile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5125428",
   "metadata": {},
   "source": [
    "#### Fit Model to Training Data\n",
    "\n",
    "When fitting the model you get to choose \n",
    "- batch size\n",
    "- epochs\n",
    "- validation split\n",
    "- class weight\n",
    "- ... \n",
    "\n",
    "The arguments of the `fit` function are elaborated in the below link\n",
    "https://keras.rstudio.com/reference/fit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- fit(\n",
    "    object = model,\n",
    "    x = train_data,\n",
    "    y = train_labels,\n",
    "    epochs = 25,\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966f860",
   "metadata": {},
   "source": [
    "#### Output of De-Normalized Data\n",
    "\n",
    "Below is the output of the denormalized data. Ideally MLP requires the data to be normalized between 0 and 1 for it to be efficient. If the data is not normalized then it will take many epochs for the model to converge to reasonably optimum weights, which will require more compute capacity. Also if the scale between the features are vastly different, chances are that it might get held up in a local minima far from the global minima.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/denorm.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f6628",
   "metadata": {},
   "source": [
    "#### Performance of MLP with Normalized Input Data\n",
    "\n",
    "On check the summary of the training data, it can be observed that the 1st ten columns' scale is very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f640d",
   "metadata": {},
   "source": [
    "One way of normalizing is by z-score -> ($x-\\mu) / \\sigma$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = d1 \n",
    "\n",
    "means1 = sapply(d1[,1:10], mean)\n",
    "sd1 = sapply(d1[,1:10], sd)\n",
    "\n",
    "d2[ , 1:10 ] = t((t(d1[ , 1:10 ]) - means1)  / sd1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6e475",
   "metadata": {},
   "source": [
    "Rest of the steps follow as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15883405",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  =  d2[  , -ncol(d2) ]\n",
    "train_labels = d2[  ,  ncol(d2) ]\n",
    "\n",
    "train_data = as.matrix(train_data)\n",
    "train_labels = to_categorical(train_labels-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential()  %>% \n",
    "\n",
    "layer_dense(\n",
    "    input_shape = ncol(train_data), \n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "layer_dense(\n",
    "    units = 200, \n",
    "    activation = 'relu'\n",
    ") %>% \n",
    "\n",
    "layer_dense(units = ncol(train_labels), activation = 'softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5119c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = optimizer_adam(),\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "history <- fit(\n",
    "    object = model,\n",
    "    x = train_data,\n",
    "    y = train_labels,\n",
    "    epochs = 25,\n",
    "    validation_split = 0.2\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd4229",
   "metadata": {},
   "source": [
    "#### Output of Normalized Data\n",
    "\n",
    "It can be observed that the model converges faster to optimum than de-normalized data.\n",
    "There are many methods of normalization. [This article](https://developers.google.com/machine-learning/data-prep/transform/normalization) provides a good explanation of the available methods.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/denorm.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f48f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909e166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc6707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf86297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c309c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147400b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
